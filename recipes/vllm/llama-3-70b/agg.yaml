# Llama 3.3-70B Aggregated with vLLM
# Based on dynamo exemplar: recipes/llama-3-70b/vllm/agg
# Configuration: 1 aggregated worker with TP4 on 4x H100 GPUs

name: "llama3-70b-vllm-agg"

model:
  path: "RedHatAI/Llama-3.3-70B-Instruct-FP8-dynamic"
  container: "nvcr.io/nvidia/ai-dynamo/vllm-runtime:0.8.0"
  precision: "fp8"

resources:
  gpu_type: "h100"
  gpus_per_node: 8
  # Aggregated mode: 1 TP4 worker = 4 GPUs
  agg_nodes: 1
  agg_workers: 1
  gpus_per_agg: 4

frontend:
  type: dynamo
  enable_multiple_frontends: false

backend:
  type: vllm
  connector: nixl

  aggregated_environment:
    PYTHONUNBUFFERED: "1"

  vllm_config:
    aggregated:
      served-model-name: "RedHatAI/Llama-3.3-70B-Instruct-FP8-dynamic"
      tensor-parallel-size: 4
      data-parallel-size: 1
      gpu-memory-utilization: 0.90
      no-enable-prefix-caching: true
      block-size: 128

benchmark:
  type: "mooncake-router"
  mooncake_workload: "conversation"
  ttft_threshold_ms: 2000
  itl_threshold_ms: 25
