# Llama 3.3-70B Disaggregated Single-Node with vLLM
# Based on dynamo exemplar: recipes/llama-3-70b/vllm/disagg-single-node
# Configuration: 2 prefill (TP2) + 1 decode (TP4) on 8x H100 GPUs (1 node)

name: "llama3-70b-vllm-disagg-sn"

model:
  path: "RedHatAI/Llama-3.3-70B-Instruct-FP8-dynamic"
  container: "nvcr.io/nvidia/ai-dynamo/vllm-runtime:0.8.0"
  precision: "fp8"

resources:
  gpu_type: "h100"
  gpus_per_node: 8
  # Single-node disaggregated:
  # 2 prefill workers @ TP2 = 4 GPUs
  # 1 decode worker @ TP4 = 4 GPUs
  # Total: 8 GPUs on 1 node
  prefill_nodes: 1
  decode_nodes: 0
  prefill_workers: 2
  decode_workers: 1
  gpus_per_prefill: 2
  gpus_per_decode: 4

frontend:
  type: dynamo
  enable_multiple_frontends: false

backend:
  type: vllm
  connector: nixl

  prefill_environment:
    PYTHONUNBUFFERED: "1"

  decode_environment:
    PYTHONUNBUFFERED: "1"

  vllm_config:
    prefill:
      served-model-name: "RedHatAI/Llama-3.3-70B-Instruct-FP8-dynamic"
      tensor-parallel-size: 2
      data-parallel-size: 1
      gpu-memory-utilization: 0.90
      no-enable-prefix-caching: true
      block-size: 128

    decode:
      served-model-name: "RedHatAI/Llama-3.3-70B-Instruct-FP8-dynamic"
      tensor-parallel-size: 4
      data-parallel-size: 1
      gpu-memory-utilization: 0.90
      no-enable-prefix-caching: true
      block-size: 128

benchmark:
  type: "mooncake-router"
  mooncake_workload: "conversation"
  ttft_threshold_ms: 2000
  itl_threshold_ms: 25
